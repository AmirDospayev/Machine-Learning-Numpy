{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "def pad_audio(samples, L=16000):\n",
    "    if len(samples) >= L: return samples\n",
    "    else: return np.pad(samples, pad_width=(L - len(samples), 0), mode='constant', constant_values=(0, 0))\n",
    "\n",
    "def chop_audio(samples, L=16000, num=20):\n",
    "    for i in range(num):\n",
    "        beg = np.random.randint(0, len(samples) - L)\n",
    "        yield samples[beg: beg + L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seven', 'one', 'five', 'nine', 'down', 'four', 'eight']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_location = os.listdir('/home/husein/Desktop/convolutional-neural-network/audio')\n",
    "audio_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "new_sample_rate = 8000\n",
    "for i in audio_location:\n",
    "    audios = os.listdir('/home/husein/Desktop/convolutional-neural-network/audio/%s'%(i))\n",
    "    for k in audios:\n",
    "        sample_rate, samples = scipy.io.wavfile.read(os.path.join('/home/husein/Desktop/convolutional-neural-network/audio', i, k))\n",
    "        samples = pad_audio(samples)\n",
    "        if len(samples) > 16000:\n",
    "            n_samples = chop_audio(samples)\n",
    "        else: \n",
    "            n_samples = [samples]\n",
    "        for samples in n_samples:\n",
    "            resampled = signal.resample(samples, int(new_sample_rate / sample_rate * samples.shape[0]))\n",
    "            _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "            Y.append(i)\n",
    "            X.append(np.expand_dims(scipy.misc.imresize(specgram,[45, 40]),axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 45, 40, 1)\n"
     ]
    }
   ],
   "source": [
    "labels = np.unique(Y)\n",
    "Y = LabelEncoder().fit_transform(Y)\n",
    "c = list(zip(X, Y))\n",
    "random.shuffle(c)\n",
    "X, Y = zip(*c)\n",
    "X, Y = np.array(X), np.array(Y)\n",
    "print(X.shape)\n",
    "onehot = np.zeros((X.shape[0],labels.shape[0]))\n",
    "onehot[np.arange(Y.shape[0]), Y] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding(filter_shape, pad='same'):\n",
    "    if pad == 'valid':\n",
    "        return (0, 0), (0, 0)\n",
    "    if pad == 'same':\n",
    "        filter_height, filter_width = filter_shape\n",
    "        pad_h_min = int(np.floor((filter_height - 1)/2))\n",
    "        pad_h_max = int(np.ceil((filter_height - 1)/2))\n",
    "        pad_w_min = int(np.floor((filter_width - 1)/2))\n",
    "        pad_w_max = int(np.ceil((filter_width - 1)/2))\n",
    "        return (pad_h_min, pad_h_max), (pad_w_min, pad_w_max)\n",
    "    \n",
    "def images_to_column_indices(images_shape, filter_shape, padding, rate, stride=1):\n",
    "    batch_size, height, width, channels = images_shape\n",
    "    filter_height, filter_width = filter_shape\n",
    "    pad_h, pad_w = padding\n",
    "    out_height = int(np.ceil((height + np.sum(pad_h) - rate * (filter_height-1)) / stride) + 1)\n",
    "    out_width = int(np.ceil((width + np.sum(pad_w) - rate * (filter_width-1)) / stride) + 1)\n",
    "    i0 = np.repeat(np.arange(filter_height), filter_width)\n",
    "    i0 = np.tile(i0, channels)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(filter_width), filter_height * channels)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    k = np.repeat(np.arange(channels), filter_height * filter_width).reshape(-1, 1)\n",
    "    return (i, j, k)\n",
    "    \n",
    "def image_to_column(images, filter_shape, rate, stride, pad='same'):\n",
    "    filter_height, filter_width = filter_shape\n",
    "    pad_h, pad_w = get_padding(filter_shape, pad)\n",
    "    images_padded = np.pad(images, ((0, 0), pad_h, pad_w, (0, 0)), mode='constant')\n",
    "    i, j, k = images_to_column_indices(images.shape, filter_shape, (pad_h, pad_w), rate, stride)\n",
    "    cols = images_padded[:, i, j, k]\n",
    "    channels = images.shape[3]\n",
    "    return cols.reshape(filter_height * filter_width * channels, -1)\n",
    "\n",
    "def get_shape(x, filter_shape, rate, stride, pad):\n",
    "    _, height, width, _ = x.shape\n",
    "    pad_h, pad_w = get_padding(filter_shape, pad)\n",
    "    output_height = int(np.ceil((height + np.sum(pad_h) - rate * (filter_shape[0]-1)) / stride) + 1)\n",
    "    output_width = int(np.ceil((width + np.sum(pad_w) - rate * (filter_shape[1]-1)) / stride) + 1)\n",
    "    return int(output_height), int(output_width)\n",
    "\n",
    "def column_to_image(cols, images_shape, filter_shape, rate, stride, pad='same'):\n",
    "    batch_size, height, width, channels = images_shape\n",
    "    pad_h, pad_w = get_padding(filter_shape, pad)\n",
    "    height_padded = height + np.sum(pad_h)\n",
    "    width_padded = width + np.sum(pad_w)\n",
    "    images_padded = np.empty((batch_size, height_padded, width_padded, channels))\n",
    "    i, j, k = images_to_column_indices(images_shape, filter_shape, (pad_h, pad_w), rate, stride)\n",
    "    cols = cols.reshape(channels * np.prod(filter_shape), -1, batch_size)\n",
    "    cols = cols.transpose(2,0,1)\n",
    "    np.add.at(images_padded, (slice(None), i, j, k), cols)\n",
    "    return images_padded[:, pad_h[0]:height+pad_h[0], pad_w[0]:width+pad_w[0], :]\n",
    "\n",
    "def conv_forward(X, W, rate, stride=1, pad='same'):\n",
    "    filter_shape = (W.shape[0], W.shape[1])\n",
    "    n_filter = W.shape[3]\n",
    "    X_col = image_to_column(X, filter_shape, rate, stride=stride, pad=pad)\n",
    "    W_col = W.reshape((n_filter, -1))\n",
    "    output = W_col.dot(X_col)\n",
    "    out=get_shape(X, filter_shape, rate, stride, pad)\n",
    "    return output.reshape((X.shape[0],out[0],out[1],n_filter)), (filter_shape, n_filter, X_col, W_col,stride, pad, rate)\n",
    "\n",
    "def conv_backward(X, W, dout, cached):\n",
    "    filter_shape, n_filter, X_col, W_col,stride, pad, rate = cached\n",
    "    db = np.sum(dout, axis=(0, 1, 2)).reshape(n_filter)\n",
    "    dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n",
    "    dW = dout_reshaped.dot(X_col.T)\n",
    "    dW = dW.reshape(W.shape)\n",
    "    W_reshape = W.reshape(n_filter, -1)\n",
    "    dX_col = W_reshape.T.dot(dout_reshaped)\n",
    "    dX = column_to_image(dX_col, X.shape, filter_shape, rate, stride, pad)\n",
    "    return dX, dW, db\n",
    "\n",
    "def cross_entropy(Y_hat, Y, epsilon=1e-12):\n",
    "    Y_hat = np.clip(Y_hat, epsilon, 1. - epsilon)\n",
    "    N = Y_hat.shape[0]\n",
    "    return -np.sum(np.sum(Y * np.log(Y_hat+1e-9))) / N\n",
    "\n",
    "def softmax(x):\n",
    "    exp_scores = np.exp(x - np.max(x))\n",
    "    return exp_scores / (np.sum(exp_scores, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "def relu_forward(X):\n",
    "    out = np.maximum(X, 0)\n",
    "    cached = X\n",
    "    return out, cached\n",
    "\n",
    "def relu_backward(X, cached):\n",
    "    X[cached <= 0] = 0\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size = 3\n",
    "rate = 3\n",
    "stride = 2\n",
    "epoch = 20\n",
    "learning_rate = 0.00001\n",
    "starting_dimension = X[0].shape[2]\n",
    "kernel_1 = np.random.randn(filter_size, filter_size, starting_dimension, 16) / np.sqrt(starting_dimension)\n",
    "bias_1 = np.zeros(16)\n",
    "kernel_2 = np.random.randn(filter_size, filter_size, 16, 32) / np.sqrt(16)\n",
    "bias_2 = np.zeros(32)\n",
    "kernel_3 = np.random.randn(filter_size, filter_size, 32, 64) / np.sqrt(32)\n",
    "bias_3 = np.zeros(64)\n",
    "w_1 = None\n",
    "b_1 = np.zeros(128)\n",
    "w_2 = np.random.randn(128, labels.shape[0]) / np.sqrt(128)\n",
    "b_2 = np.zeros(labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 20.722266, accuracy 0.114286\n",
      "epoch 2, loss 20.426234, accuracy 0.157143\n",
      "epoch 3, loss 1.946156, accuracy 0.142857\n",
      "epoch 4, loss 1.945995, accuracy 0.142857\n",
      "epoch 5, loss 1.945940, accuracy 0.142857\n",
      "epoch 6, loss 1.945921, accuracy 0.142857\n",
      "epoch 7, loss 1.945915, accuracy 0.142857\n",
      "epoch 8, loss 1.945913, accuracy 0.142857\n",
      "epoch 9, loss 1.945912, accuracy 0.142857\n",
      "epoch 10, loss 1.945911, accuracy 0.142857\n",
      "epoch 11, loss 1.945911, accuracy 0.142857\n",
      "epoch 12, loss 1.945911, accuracy 0.142857\n",
      "epoch 13, loss 1.945911, accuracy 0.142857\n",
      "epoch 14, loss 1.945910, accuracy 0.142857\n",
      "epoch 15, loss 1.945910, accuracy 0.142857\n",
      "epoch 16, loss 1.945910, accuracy 0.142857\n",
      "epoch 17, loss 1.945910, accuracy 0.142857\n",
      "epoch 18, loss 1.945910, accuracy 0.142857\n",
      "epoch 19, loss 1.945910, accuracy 0.142857\n",
      "epoch 20, loss 1.945910, accuracy 0.142857\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    conv1, cached1 = conv_forward(X, kernel_1, rate, stride)\n",
    "    conv1 = conv1 + bias_1\n",
    "    z1, relu_cached1 = relu_forward(conv1)\n",
    "    conv2, cached2 = conv_forward(z1, kernel_2, rate, stride)\n",
    "    conv2 = conv2 + bias_2\n",
    "    z2, relu_cached2 = relu_forward(conv2)\n",
    "    conv3, cached3 = conv_forward(z2, kernel_3, rate, stride)\n",
    "    conv3 = conv3 + bias_3\n",
    "    z3, relu_cached3 = relu_forward(conv3)\n",
    "    h, w = z3.shape[1], z3.shape[2]\n",
    "    z3_reshape = z3.reshape((-1, h * w * 64))\n",
    "    if w_1 is None:\n",
    "        w_1 = np.random.randn(h * w * 64, 128) / np.sqrt(h * w * 64)\n",
    "    fully1 = np.dot(z3_reshape, w_1) + b_1\n",
    "    z4, relu_cached4 = relu_forward(fully1)\n",
    "    logits = np.dot(z4, w_2) + b_2\n",
    "    probs = softmax(logits)\n",
    "    accuracy = np.mean(np.argmax(probs,axis=1) == Y)\n",
    "    loss = cross_entropy(probs, onehot)\n",
    "    delta = probs\n",
    "    delta[range(Y.shape[0]), Y] -= 1\n",
    "    dw_2 = np.dot(z4.T, delta)\n",
    "    db_2 = np.sum(delta,axis=0)\n",
    "    dz4 = np.dot(delta,w_2.T)\n",
    "    dfully1 = relu_backward(dz4, relu_cached4)\n",
    "    dw_1 = np.dot(z3_reshape.T, dfully1)\n",
    "    db_1 = np.sum(dfully1,axis=0)\n",
    "    dz3_reshape = np.dot(dfully1,w_1.T)\n",
    "    dz3 = dz3_reshape.reshape((-1, h, w, 64))\n",
    "    dconv3 = relu_backward(dz3, relu_cached3)\n",
    "    dz2, dkernel_3, dbias_3 = conv_backward(z2, kernel_3, dconv3, cached3)\n",
    "    dconv2 = relu_backward(dz2, relu_cached2)\n",
    "    dz1, dkernel_2, dbias_2 = conv_backward(z1, kernel_2, dconv2, cached2)\n",
    "    dconv1 = relu_backward(dz1, relu_cached1)\n",
    "    _, dkernel_1, dbias_1 = conv_backward(X, kernel_1, dconv1, cached1)\n",
    "    kernel_1 -= learning_rate * dkernel_1\n",
    "    bias_1 -= learning_rate * dbias_1\n",
    "    kernel_2 -= learning_rate * dkernel_2\n",
    "    bias_2 -= learning_rate * dbias_2\n",
    "    kernel_3 -= learning_rate * dkernel_3\n",
    "    bias_3 -= learning_rate * dbias_3\n",
    "    w_1 -= learning_rate * dw_1\n",
    "    b_1 -= learning_rate * db_1\n",
    "    w_2 -= learning_rate * dw_2\n",
    "    b_2 -= learning_rate * db_2\n",
    "    print('epoch %d, loss %f, accuracy %f'%(i+1, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
