{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(file, lower = False):\n",
    "    with open(file, 'r') as fopen:\n",
    "        data = fopen.read()\n",
    "    if lower:\n",
    "        data = data.lower()\n",
    "    vocab = list(set(data))\n",
    "    return data, vocab\n",
    "\n",
    "def embed_to_onehot(data, vocab):\n",
    "    onehot = np.zeros((len(data), len(vocab)), dtype = np.float32)\n",
    "    for i in range(len(data)):\n",
    "        onehot[i, vocab.index(data[i])] = 1.0\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, text_vocab = get_vocab('consumer.h', lower = False)\n",
    "onehot = embed_to_onehot(text, text_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "sequence_length = 12\n",
    "epoch = 1000\n",
    "num_layers = 2\n",
    "size_layer = 128\n",
    "possible_batch_id = range(len(text) - sequence_length - 1)\n",
    "dimension = onehot.shape[1]\n",
    "\n",
    "U = np.random.randn(size_layer, dimension) / np.sqrt(size_layer)\n",
    "Wz = np.random.randn(size_layer, size_layer) / np.sqrt(size_layer)\n",
    "Wr = np.random.randn(size_layer, size_layer) / np.sqrt(size_layer)\n",
    "Wh = np.random.randn(size_layer, size_layer) / np.sqrt(size_layer)\n",
    "V = np.random.randn(dimension, size_layer) / np.sqrt(dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, grad=False):\n",
    "    if grad:\n",
    "        output = np.tanh(x)\n",
    "        return (1.0 - np.square(output))\n",
    "    else:\n",
    "        return np.tanh(x)\n",
    "    \n",
    "def sigmoid(x, grad=False):\n",
    "    if grad:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def softmax(x):\n",
    "    exp_scores = np.exp(x - np.max(x))\n",
    "    return exp_scores / (np.sum(exp_scores, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "def derivative_softmax_cross_entropy(x, y):\n",
    "    delta = softmax(x)\n",
    "    delta[range(X.shape[0]), y] -= 1\n",
    "    return delta\n",
    "\n",
    "def forward_multiply_gate(w, x):\n",
    "    return np.dot(w, x)\n",
    "\n",
    "def backward_multiply_gate(w, x, dz):\n",
    "    dW = np.dot(dz.T, x)\n",
    "    dx = np.dot(w.T, dz.T)\n",
    "    return dW, dx\n",
    "\n",
    "def forward_add_gate(x1, x2):\n",
    "    return x1 + x2\n",
    "\n",
    "def backward_add_gate(x1, x2, dz):\n",
    "    dx1 = dz * np.ones_like(x1)\n",
    "    dx2 = dz * np.ones_like(x2)\n",
    "    return dx1, dx2\n",
    "\n",
    "def cross_entropy(Y_hat, Y, epsilon=1e-12):\n",
    "    Y_hat = np.clip(Y_hat, epsilon, 1. - epsilon)\n",
    "    N = Y_hat.shape[0]\n",
    "    return -np.sum(np.sum(Y * np.log(Y_hat+1e-9))) / N\n",
    "\n",
    "def forward_recurrent(x, h_state, U, Wz, Wr, Wh, V):\n",
    "    mul_u = forward_multiply_gate(x, U.T)\n",
    "    mul_Wz = forward_multiply_gate(h_state, Wz.T)\n",
    "    add_Wz = forward_add_gate(mul_u, mul_Wz)\n",
    "    z = sigmoid(add_Wz)\n",
    "    mul_Wr = forward_multiply_gate(h_state, Wr.T)\n",
    "    add_Wr = forward_add_gate(mul_u, mul_Wr)\n",
    "    r = sigmoid(add_Wr)\n",
    "    mul_Wh = forward_multiply_gate(h_state * r, Wh.T)\n",
    "    add_Wh = forward_add_gate(mul_u, mul_Wh)\n",
    "    h_hat = tanh(add_Wh)\n",
    "    h = (1 - z) * h_state + z * h_hat\n",
    "    mul_v = forward_multiply_gate(h, V.T)\n",
    "    return (mul_u, mul_Wz, add_Wz, z, mul_Wr, add_Wr, r, mul_Wh, add_Wh, h_hat, h, mul_v)\n",
    "\n",
    "def backward_recurrent(x, h_state, U, Wz, Wr, Wh, V, d_mul_v, saved_graph):\n",
    "    mul_u, mul_Wz, add_Wz, z, mul_Wr, add_Wr, r, mul_Wh, add_Wh, h_hat, h, mul_v = saved_graph\n",
    "    dV, dh = backward_multiply_gate(V, h, d_mul_v)\n",
    "    dh_hat = z * dh.T\n",
    "    dadd_Wh = tanh(add_Wh, True) * dh_hat\n",
    "    dmul_u1, dmul_Wh = backward_add_gate(mul_u, mul_Wh, dadd_Wh)\n",
    "    dWh, dprev_state = backward_multiply_gate(Wh, h_state * r, dmul_Wh)\n",
    "    dr = dprev_state * h_state.T\n",
    "    dadd_Wr = sigmoid(add_Wr, True) * dr.T\n",
    "    dmul_u2, dmul_Wr = backward_add_gate(mul_u, mul_Wr, dadd_Wr)\n",
    "    dWr, dprev_state = backward_multiply_gate(Wr, h_state, dmul_Wr)\n",
    "    dz = -h_state + h_hat\n",
    "    dadd_Wz = sigmoid(add_Wz, True) * dz\n",
    "    dmul_u3, dmul_Wz = backward_add_gate(mul_u, mul_Wz, dadd_Wz)\n",
    "    dWz, dprev_state = backward_multiply_gate(Wz, h_state, dmul_Wz)\n",
    "    dU, dx = backward_multiply_gate(U, x, dmul_u3)\n",
    "    return (dU, dWz, dWr, dWh, dV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 4.220839, accuracy 0.223958\n",
      "epoch 100, loss 4.151175, accuracy 0.196615\n",
      "epoch 150, loss 3.968643, accuracy 0.158854\n",
      "epoch 200, loss 4.017142, accuracy 0.191406\n",
      "epoch 250, loss 4.093218, accuracy 0.182292\n",
      "epoch 300, loss 4.048298, accuracy 0.134115\n",
      "epoch 350, loss 3.878078, accuracy 0.130208\n",
      "epoch 400, loss 3.772771, accuracy 0.085938\n",
      "epoch 450, loss 3.726045, accuracy 0.106771\n",
      "epoch 500, loss 3.694129, accuracy 0.109375\n",
      "epoch 550, loss 3.727348, accuracy 0.075521\n",
      "epoch 600, loss 3.521524, accuracy 0.095052\n",
      "epoch 650, loss 3.571376, accuracy 0.106771\n",
      "epoch 700, loss 3.573480, accuracy 0.085938\n",
      "epoch 750, loss 3.590403, accuracy 0.123698\n",
      "epoch 800, loss 3.614294, accuracy 0.111979\n",
      "epoch 850, loss 3.564591, accuracy 0.123698\n",
      "epoch 900, loss 3.591091, accuracy 0.111979\n",
      "epoch 950, loss 3.543047, accuracy 0.105469\n",
      "epoch 1000, loss 3.542467, accuracy 0.102865\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)\n",
    "    prev_h = np.zeros((batch_size, size_layer))\n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        batch_x[:,n,:] = onehot[id1, :]\n",
    "        batch_y[:,n,:] = onehot[id2, :]\n",
    "    layers = []\n",
    "    out_logits = np.zeros((batch_size, sequence_length, dimension))\n",
    "    for n in range(sequence_length):\n",
    "        layers.append(forward_recurrent(batch_x[:,n,:], prev_h, U, Wz, Wr, Wh, V))\n",
    "        prev_h = layers[-1][-2]\n",
    "        out_logits[:, n, :] = layers[-1][-1]\n",
    "    probs = softmax(out_logits.reshape((-1, dimension)))\n",
    "    y = np.argmax(batch_y.reshape((-1, dimension)),axis=1)\n",
    "    accuracy = np.mean(np.argmax(probs,axis=1) == y)\n",
    "    loss = cross_entropy(probs, batch_y.reshape((-1, dimension)))\n",
    "    delta = probs\n",
    "    delta[range(y.shape[0]), y] -= 1\n",
    "    delta = delta.reshape((batch_size, sequence_length, dimension))\n",
    "    dU = np.zeros(U.shape)\n",
    "    dV = np.zeros(V.shape)\n",
    "    dWz = np.zeros(Wz.shape)\n",
    "    dWr = np.zeros(Wr.shape)\n",
    "    dWh = np.zeros(Wh.shape)\n",
    "    prev_h = np.zeros((batch_size, size_layer))\n",
    "    for n in range(sequence_length):\n",
    "        d_mul_v = delta[:, n, :]\n",
    "        dU_t, dWz_t, dWr_t, dWh_t, dV_t = backward_recurrent(batch_x[:,n,:], prev_h, \n",
    "                                                                    U, Wz, Wr, Wh, V, d_mul_v, layers[n])\n",
    "        prev_h = layers[n][-2]\n",
    "        dU += dU_t\n",
    "        dV += dV_t\n",
    "        dWz += dWz_t\n",
    "        dWr += dWr_t\n",
    "        dWh += dWh_t\n",
    "    U -= learning_rate * dU\n",
    "    V -= learning_rate * dV\n",
    "    Wz -= learning_rate * dWz\n",
    "    Wr -= learning_rate * dWr\n",
    "    Wh -= learning_rate * dWh\n",
    "    if (i+1) % 50 == 0:\n",
    "        print('epoch %d, loss %f, accuracy %f'%(i+1, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
