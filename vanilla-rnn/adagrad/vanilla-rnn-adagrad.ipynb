{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(file, lower = False):\n",
    "    with open(file, 'r') as fopen:\n",
    "        data = fopen.read()\n",
    "    if lower:\n",
    "        data = data.lower()\n",
    "    vocab = list(set(data))\n",
    "    return data, vocab\n",
    "\n",
    "def embed_to_onehot(data, vocab):\n",
    "    onehot = np.zeros((len(data), len(vocab)), dtype = np.float32)\n",
    "    for i in range(len(data)):\n",
    "        onehot[i, vocab.index(data[i])] = 1.0\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, text_vocab = get_vocab('consumer.h', lower = False)\n",
    "onehot = embed_to_onehot(text, text_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "sequence_length = 12\n",
    "epoch = 1000\n",
    "num_layers = 2\n",
    "size_layer = 128\n",
    "possible_batch_id = range(len(text) - sequence_length - 1)\n",
    "dimension = onehot.shape[1]\n",
    "epsilon = 1e-8\n",
    "\n",
    "U = np.random.randn(size_layer, dimension) / np.sqrt(size_layer)\n",
    "U_g = np.zeros(U.shape)\n",
    "W = np.random.randn(size_layer, size_layer) / np.sqrt(size_layer)\n",
    "W_g = np.zeros(W.shape)\n",
    "V = np.random.randn(dimension, size_layer) / np.sqrt(dimension)\n",
    "V_g = np.zeros(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, grad=False):\n",
    "    if grad:\n",
    "        output = np.tanh(x)\n",
    "        return (1.0 - np.square(output))\n",
    "    else:\n",
    "        return np.tanh(x)\n",
    "    \n",
    "def softmax(x):\n",
    "    exp_scores = np.exp(x - np.max(x))\n",
    "    return exp_scores / (np.sum(exp_scores, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "def derivative_softmax_cross_entropy(x, y):\n",
    "    delta = softmax(x)\n",
    "    delta[range(X.shape[0]), y] -= 1\n",
    "    return delta\n",
    "\n",
    "def forward_multiply_gate(w, x):\n",
    "    return np.dot(w, x)\n",
    "\n",
    "def backward_multiply_gate(w, x, dz):\n",
    "    dW = np.dot(dz.T, x)\n",
    "    dx = np.dot(w.T, dz.T)\n",
    "    return dW, dx\n",
    "\n",
    "def forward_add_gate(x1, x2):\n",
    "    return x1 + x2\n",
    "\n",
    "def backward_add_gate(x1, x2, dz):\n",
    "    dx1 = dz * np.ones_like(x1)\n",
    "    dx2 = dz * np.ones_like(x2)\n",
    "    return dx1, dx2\n",
    "\n",
    "def cross_entropy(Y_hat, Y, epsilon=1e-12):\n",
    "    Y_hat = np.clip(Y_hat, epsilon, 1. - epsilon)\n",
    "    N = Y_hat.shape[0]\n",
    "    return -np.sum(np.sum(Y * np.log(Y_hat+1e-9))) / N\n",
    "\n",
    "def forward_recurrent(x, prev_state, U, W, V):\n",
    "    mul_u = forward_multiply_gate(x, U.T)\n",
    "    mul_w = forward_multiply_gate(prev_state, W.T)\n",
    "    add_previous_now = forward_add_gate(mul_u, mul_w)\n",
    "    current_state = tanh(add_previous_now)\n",
    "    mul_v = forward_multiply_gate(current_state, V.T)\n",
    "    return (mul_u, mul_w, add_previous_now, current_state, mul_v)\n",
    "\n",
    "def backward_recurrent(x, prev_state, U, W, V, d_mul_v, saved_graph):\n",
    "    mul_u, mul_w, add_previous_now, current_state, mul_v = saved_graph\n",
    "    dV, dcurrent_state = backward_multiply_gate(V, current_state, d_mul_v)\n",
    "    dadd_previous_now = tanh(add_previous_now, True) * dcurrent_state.T\n",
    "    dmul_w, dmul_u = backward_add_gate(mul_w, mul_u, dadd_previous_now)\n",
    "    dW, dprev_state = backward_multiply_gate(W, prev_state, dmul_w)\n",
    "    dU, dx = backward_multiply_gate(U, x, dmul_u)\n",
    "    return (dprev_state, dU, dW, dV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 2.192045, accuracy 0.490885\n",
      "epoch 100, loss 1.801595, accuracy 0.571615\n",
      "epoch 150, loss 1.777153, accuracy 0.559896\n",
      "epoch 200, loss 1.680624, accuracy 0.583333\n",
      "epoch 250, loss 1.381813, accuracy 0.658854\n",
      "epoch 300, loss 1.309808, accuracy 0.673177\n",
      "epoch 350, loss 1.257366, accuracy 0.684896\n",
      "epoch 400, loss 1.152840, accuracy 0.718750\n",
      "epoch 450, loss 1.214343, accuracy 0.675781\n",
      "epoch 500, loss 1.187885, accuracy 0.690104\n",
      "epoch 550, loss 1.129081, accuracy 0.725260\n",
      "epoch 600, loss 1.072351, accuracy 0.718750\n",
      "epoch 650, loss 1.180679, accuracy 0.701823\n",
      "epoch 700, loss 1.060361, accuracy 0.743490\n",
      "epoch 750, loss 1.124037, accuracy 0.720052\n",
      "epoch 800, loss 1.187253, accuracy 0.712240\n",
      "epoch 850, loss 1.157658, accuracy 0.717448\n",
      "epoch 900, loss 0.906449, accuracy 0.772135\n",
      "epoch 950, loss 1.138607, accuracy 0.714844\n",
      "epoch 1000, loss 0.915181, accuracy 0.751302\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)\n",
    "    prev_s = np.zeros((batch_size, size_layer))\n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        batch_x[:,n,:] = onehot[id1, :]\n",
    "        batch_y[:,n,:] = onehot[id2, :]\n",
    "    layers = []\n",
    "    out_logits = np.zeros((batch_size, sequence_length, dimension))\n",
    "    for n in range(sequence_length):\n",
    "        layers.append(forward_recurrent(batch_x[:,n,:], prev_s, U, W, V))\n",
    "        prev_s = layers[-1][3]\n",
    "        out_logits[:, n, :] = layers[-1][-1]\n",
    "    probs = softmax(out_logits.reshape((-1, dimension)))\n",
    "    y = np.argmax(batch_y.reshape((-1, dimension)),axis=1)\n",
    "    accuracy = np.mean(np.argmax(probs,axis=1) == y)\n",
    "    loss = cross_entropy(probs, batch_y.reshape((-1, dimension)))\n",
    "    delta = probs\n",
    "    delta[range(y.shape[0]), y] -= 1\n",
    "    delta = delta.reshape((batch_size, sequence_length, dimension))\n",
    "    dU = np.zeros(U.shape)\n",
    "    dV = np.zeros(V.shape)\n",
    "    dW = np.zeros(W.shape)\n",
    "    prev_state = np.zeros((batch_size, size_layer))\n",
    "    for n in range(sequence_length):\n",
    "        d_mul_v = delta[:, n, :]\n",
    "        dprev_s, dU_t, dW_t, dV_t = backward_recurrent(batch_x[:,n,:], prev_state, U, W, V, d_mul_v, layers[n])\n",
    "        prev_state = layers[n][3]\n",
    "        dV += dV_t\n",
    "        dU += dU_t\n",
    "        dW += dW_t\n",
    "    U_g += dU ** 2\n",
    "    U += -learning_rate * dU / np.sqrt(U_g + epsilon)\n",
    "    V_g += dV ** 2\n",
    "    V += -learning_rate * dV / np.sqrt(V_g + epsilon)\n",
    "    W_g += dW ** 2\n",
    "    W += -learning_rate * dW / np.sqrt(W_g + epsilon)\n",
    "    if (i+1) % 50 == 0:\n",
    "        print('epoch %d, loss %f, accuracy %f'%(i+1, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
